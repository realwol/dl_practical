{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章节的练习中，首先我们要完成Momentum，RMSProp，Adam三种优化方法的代码编写。在此之后，我们将重点进行BN算法的前向传播，反向传播的实现。本章我们将逐步完成：\n",
    "* 编码实现Momentum算法；\n",
    "* 编码实现RMSProp算法；\n",
    "* 编码实现Adam算法；\n",
    "* 编码实现BN前向传播；\n",
    "* 编码实现BN反向传播；\n",
    "* 编码实现BN全连接网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from classifiers.chapter5 import *\n",
    "from utils import *\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" 返回相对误差 \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "打开文件 `classifiers\\chapter5\\updater.py` 文件，完成 `sgd_momentum` 函数编码后，运行下列代码检验。你的相对误差应该小于1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print '更新权重误差: ', rel_error(next_w, expected_next_w)\n",
    "print '速度误差: ', rel_error(expected_velocity, config['velocity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你实现 `momentum` 更新规则后，我们使用5层隐藏层神经网络进行测试，正常情况下，加入动量方法后会比原始的SGD算法收敛得更快一些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "trainers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  model = FullyConnectedNet(hidden_dims=[100, 100, 100, 100, 100], weight_scale=7e-2)\n",
    "\n",
    "  trainer = Trainer(model, small_data,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  updater_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  verbose=False)\n",
    "  trainers[update_rule] = trainer\n",
    "  trainer.train()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss',fontsize=18)\n",
    "plt.xlabel('Iteration',fontsize=18)\n",
    "plt.ylabel('Loss',fontsize=18)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=18)\n",
    "plt.ylabel('Accuracy',fontsize=18)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=18)\n",
    "plt.ylabel('Accuracy',fontsize=18)\n",
    "plt.subplots_adjust(left=0.08, right=0.95, wspace=0.25, hspace=0.25)\n",
    "a = {'sgd':'o', 'sgd_momentum':'*'}\n",
    "for update_rule, trainer in trainers.iteritems():\n",
    "\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(trainer.loss_history, a[update_rule], label=update_rule)\n",
    "  \n",
    "\n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(trainer.train_acc_history, '-'+a[update_rule], label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(trainer.val_acc_history, '-'+a[update_rule], label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "打开 `classifiers\\chapter5\\updater.py` 文件，完成 `RMSProp` 以及 `Adam` 函数编码后，运行下列代码检验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 测试 RMSProp ;相对误差应该小于1e-7\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print '权重更新误差: ', rel_error(expected_next_w, next_w)\n",
    "print 'cache 误差: ', rel_error(expected_cache, config['cache'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 测试 Adam \n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print '权重更新误差: ', rel_error(expected_next_w, next_w)\n",
    "print 'v 误差: ', rel_error(expected_v, config['v'])\n",
    "print 'm 误差: ', rel_error(expected_m, config['m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们测试SGD，momentum，RMSProp以及Adam。由于SGD，momentum已经在trainers字典中了，我们只需要实例RMSProp以及Adam网络即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  model = FullyConnectedNet(hidden_dims=[100, 100, 100, 100, 100], weight_scale=7e-2)\n",
    "\n",
    "  trainer = Trainer(model, small_data,\n",
    "                      num_epochs=10, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  updater_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=False)\n",
    "  trainers[update_rule] = trainer\n",
    "  trainer.train()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss',fontsize=18)\n",
    "plt.xlabel('Iteration',fontsize=18)\n",
    "plt.ylabel('Loss',fontsize=18)\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=18)\n",
    "plt.ylabel('Accuracy',fontsize=18)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=18)\n",
    "plt.ylabel('Accuracy',fontsize=18)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.95, wspace=0.25, hspace=0.25)\n",
    "a['adam'] = 'D'\n",
    "a['rmsprop'] = 'v'\n",
    "for update_rule, trainer in trainers.iteritems():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(trainer.loss_history, a[update_rule], label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(trainer.train_acc_history, '-'+a[update_rule], label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(trainer.val_acc_history, '-'+a[update_rule], label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization: 前向传播\n",
    "打开 `classifiers\\chapter5\\bn_layers.py`文件，实现`batchnorm_forward`函数，执行BN算法的前向传播过程。完成后，使用下列代码进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#检验BN训练阶段前向传播的均值和方差\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print 'normalization之前:'\n",
    "print '  means: ', a.mean(axis=0)\n",
    "print '  stds: ', a.std(axis=0)\n",
    "\n",
    "# 均值应该接近零，标准差接近1\n",
    "print 'batch normalization之后 (gamma=1, beta=0)'\n",
    "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
    "print '  mean: ', a_norm.mean(axis=0)\n",
    "print '  std: ', a_norm.std(axis=0)\n",
    "\n",
    "# 均值应该接近beta，标准差接近gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print ' batch normalization之后 (随机 gamma, beta)'\n",
    "print '  means: ', a_norm.mean(axis=0)\n",
    "print '  stds: ', a_norm.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#检验测试阶段前向传播\n",
    "#注意：需要训练一段时间后运行时均值才会稳定\n",
    "\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "for t in xrange(50):\n",
    "  X = np.random.randn(N, D1)\n",
    "  a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "  batchnorm_forward(a, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "#均值应该接近0，标准差接近1。由于使用运行时均值，可能会带有一点噪声\n",
    "print 'batch normalization之后 (测试阶段):'\n",
    "print '  means: ', a_norm.mean(axis=0)\n",
    "print '  stds: ', a_norm.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization: 反向传播\n",
    "打开 `classifiers\\chapter5\\bn_layers.py`文件，实现batchnorm_backward函数，执行BN算法的反向传播过程。完成后，使用下列代码进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 检验BN反向传播梯度\n",
    "\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "print 'dx 误差: ', rel_error(dx_num, dx)\n",
    "print 'dgamma误差: ', rel_error(da_num, dgamma)\n",
    "print 'dbeta 误差: ', rel_error(db_num, dbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization: 可选的反向传播\n",
    " 打开`classifiers\\chapter5\\bn_layers.py` 文件，阅读 `batchnorm_backward_alt`并执行下列代码，该执行效率应该要比`batchnorm_backward`快速许多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D = 100, 500\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "t1 = time.time()\n",
    "dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
    "t2 = time.time()\n",
    "dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
    "t3 = time.time()\n",
    "\n",
    "print 'dx 误差: ', rel_error(dx1, dx2)\n",
    "print 'dgamma 误差: ', rel_error(dgamma1, dgamma2)\n",
    "print 'dbeta 误差: ', rel_error(dbeta1, dbeta2)\n",
    "print '加速: %.2fx' % ((t2 - t1) / (t3 - t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用BN的全连接网络\n",
    "打开`classifiers\\chapter5\\fc_net.py`文件，实现BN算法的全连接网络。该网络将有四种选择:\n",
    "1.\t深层全连接\n",
    "2.\t全连接+dropout\n",
    "3.\t全连接+BN\n",
    "4.\t全连接+BN+dropout\n",
    "\n",
    "提示：你可以先实现`affine_BN_relu`函数，以及`affine_BN_relu_drop`函数这样有利于降低编码复杂度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print '检验 reg = ', reg\n",
    "  model = FullyConnectedNet(hidden_dims=[H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2,use_batchnorm=True)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print '初始化 loss: ', loss\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print '%s 相对误差: %.2e' % (name, rel_error(grad_num, grads[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN性能测试\n",
    "运行下列代码，测试BN算法性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用BN训练深层神经网络\n",
    "hidden = [100, 100, 100, 100, 100]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 2e-2\n",
    "bn_model = FullyConnectedNet(hidden_dims=hidden, weight_scale=weight_scale, use_batchnorm=True)\n",
    "model = FullyConnectedNet(hidden_dims=hidden, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "bn_trainer = Trainer(bn_model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                updater_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "bn_trainer.train()\n",
    "\n",
    "trainer = Trainer(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                updater_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=200)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots_adjust(left=0.08, right=0.95, wspace=0.25, hspace=0.3)\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss',fontsize=18)\n",
    "plt.xlabel('Iteration',fontsize=18)\n",
    "plt.ylabel('Loss',fontsize=18)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=18)\n",
    "plt.ylabel('Accuracy',fontsize=18)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy',fontsize=18)\n",
    "plt.xlabel('Epoch',fontsize=18)\n",
    "plt.ylabel('Accuracy',fontsize=18)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(trainer.loss_history, '*', label='baseline')\n",
    "plt.plot(bn_trainer.loss_history, 'D', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(trainer.train_acc_history, '-*', label='baseline')\n",
    "plt.plot(bn_trainer.train_acc_history, '-D', label='batchnorm')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(trainer.val_acc_history, '-*', label='baseline')\n",
    "plt.plot(bn_trainer.val_acc_history, '-D', label='batchnorm')\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN与权重初始化比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden = [50, 50, 50, 50, 50, 50, 50]\n",
    "\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "bn_trainers = {}\n",
    "trainers = {}\n",
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "t1 = time.time()\n",
    "\n",
    "for i, weight_scale in enumerate(weight_scales):\n",
    "  print 'Running weight scale %d / %d' % (i + 1, len(weight_scales))\n",
    "  bn_model = FullyConnectedNet(hidden_dims=hidden, weight_scale=weight_scale, use_batchnorm=True)\n",
    "  model = FullyConnectedNet(hidden_dims=hidden, weight_scale=weight_scale, use_batchnorm=False)\n",
    "\n",
    "  bn_trainer = Trainer(bn_model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  updater_config={\n",
    "                    'learning_rate': 3e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  bn_trainer.train()\n",
    "  bn_trainers[weight_scale] = bn_trainer\n",
    "\n",
    "  trainer = Trainer(model, small_data,\n",
    "                  num_epochs=10, batch_size=50,\n",
    "                  update_rule='adam',\n",
    "                  updater_config={\n",
    "                    'learning_rate': 3e-3,\n",
    "                  },\n",
    "                  verbose=False, print_every=200)\n",
    "  trainer.train()\n",
    "  trainers[weight_scale] = trainer\n",
    "t2 = time.time()\n",
    "print 'time: %.2f' % (t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_train_accs, bn_best_train_accs = [], []\n",
    "best_val_accs, bn_best_val_accs = [], []\n",
    "final_train_loss, bn_final_train_loss = [], []\n",
    "\n",
    "for ws in weight_scales:\n",
    "  best_train_accs.append(max(trainers[ws].train_acc_history))\n",
    "  bn_best_train_accs.append(max(bn_trainers[ws].train_acc_history))\n",
    "  \n",
    "  best_val_accs.append(max(trainers[ws].val_acc_history))\n",
    "  bn_best_val_accs.append(max(bn_trainers[ws].val_acc_history))\n",
    "  \n",
    "  final_train_loss.append(np.mean(trainers[ws].loss_history[-100:]))\n",
    "  bn_final_train_loss.append(np.mean(bn_trainers[ws].loss_history[-100:]))\n",
    "    \n",
    "plt.subplots_adjust(left=0.08, right=0.95, wspace=0.25, hspace=0.3)\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best val accuracy vs weight initialization scale',fontsize=18)\n",
    "plt.xlabel('Weight initialization scale',fontsize=18)\n",
    "plt.ylabel('Best val accuracy',fontsize=18)\n",
    "plt.semilogx(weight_scales, best_val_accs, '-D', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_val_accs, '-*', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Best train accuracy vs weight initialization scale',fontsize=18)\n",
    "plt.xlabel('Weight initialization scale',fontsize=18)\n",
    "plt.ylabel('Best training accuracy',fontsize=18)\n",
    "plt.semilogx(weight_scales, best_train_accs, '-D', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_best_train_accs, '-*', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Final training loss vs weight initialization scale',fontsize=18)\n",
    "plt.xlabel('Weight initialization scale',fontsize=18)\n",
    "plt.ylabel('Final training loss',fontsize=18)\n",
    "plt.semilogx(weight_scales, final_train_loss, '-D', label='baseline')\n",
    "plt.semilogx(weight_scales, bn_final_train_loss, '-*', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
