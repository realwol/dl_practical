{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "在本章节的练习中，我们将对卷积，池化，空间批量归一化等内容进行编码，并比较不同实现方式的执行效率。由于python语言运行效率太过缓慢，我们会在第八章的tensorflow介绍章节再次训练卷积网络，本章我们只需要针对卷积网络的各个模块进行编码练习即可。本章我们将逐步完成：\n",
    "* 卷积前向传播编码练习\n",
    "* 卷积反向传播编码练习\n",
    "* 最大池化前向传播编码练习\n",
    "* 最大池化反向传播编码练习\n",
    "* 组合完整卷积层编码练习\n",
    "* 空间批量归一化编码练习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from classifiers.chapter6 import *\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" 相对误差 \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积前向传播(简单版本)\n",
    "\n",
    "打开 `classifiters\\chapter6\\cnn_layers.py` 文件完成`conv_forward_naive`函数，实现卷积层的前向传播，目前并不需要考虑效率的问题，只要编码成功即可。完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]]])\n",
    "\n",
    "# 实现误差应该在1e-8左右\n",
    "print '测试 conv_forward_naive函数'\n",
    "print '误差: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "print a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层反向传播（简单版本）\n",
    "\n",
    "打开 `classifiters\\chapter6\\cnn_layers.py` 文件完成 `conv_backward_naive`函数，实现卷积层的反向传播，目前并不需要考虑效率的问题，只要编码成功即可。完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(\n",
    "        x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(\n",
    "        x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(\n",
    "        x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# 相对错误大约为1e-9'\n",
    "print '测试 conv_backward_naive 函数'\n",
    "print 'dx 误差: ', rel_error(dx, dx_num)\n",
    "print 'dw 误差: ', rel_error(dw, dw_num)\n",
    "print 'db 误差: ', rel_error(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大池化前向传播(简单版)\n",
    "\n",
    "打开 `classifiters\\chapter6\\cnn_layers.py` 文件,完成 `max_pool_forward_naive`函数，实现最大池化前向传播，目前并不需要考虑效率的问题，只要编码成功即可。完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# 相对误差大约为 1e-8.\n",
    "print '测试 max_pool_forward_naive 函数:'\n",
    "print '误差: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大池化反向传播(简单版本)\n",
    "打开 `classifiters\\chapter6\\cnn_layers.py` 文件,完成 `max_pool_backward_naive`函数，实现最大池化反向传播，目前并不需要考虑效率的问题，只要编码成功即可。完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(\n",
    "    lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# 相对误差大约为 1e-12\n",
    "print '测试 max_pool_backward_naive 函数:'\n",
    "print 'dx 误差: ', rel_error(dx, dx_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速卷积\n",
    "执行下列代码，比较快速卷积与你实现的卷积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classifiers.chapter6.cnn_layers import conv_forward_fast\n",
    "from time import time\n",
    "\n",
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print '测试 conv_forward_fast:'\n",
    "print '慢速版本: %fs' % (t1 - t0)\n",
    "print '快速版本: %fs' % (t2 - t1)\n",
    "print '加速: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print '误差: ', rel_error(out_naive, out_fast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速池化操作\n",
    "执行下列代码，比较快速池化与你实现的池化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print '测试 pool_forward_fast:'\n",
    "print '慢速版本: %fs' % (t1 - t0)\n",
    "print '快速版本: %fs' % (t2 - t1)\n",
    "print '加速: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print '误差: ', rel_error(out_naive, out_fast)\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print '\\n测试 pool_backward_fast:'\n",
    "print '慢速版本: %fs' % (t1 - t0)\n",
    "print '快速版本: %fs' % (t2 - t1)\n",
    "print '加速: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'dx 误差: ', rel_error(dx_naive, dx_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整的卷积层\n",
    "打开 `classifiters\\chapter6\\cnn_layers.py` 文件,阅读`conv_relu_pool_forward`,`conv_relu_pool_backward`完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_relu_pool_forward(x, w, b, conv_param, pool_param)\n",
    "dx, dw, db = conv_relu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_pool_forward(\n",
    "        x, w, b, conv_param, pool_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_pool_forward(\n",
    "        x, w, b, conv_param, pool_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_pool_forward(\n",
    "        x, w, b, conv_param, pool_param)[0], b, dout)\n",
    "\n",
    "print '测试 conv_relu_pool'\n",
    "print 'dx 误差: ', rel_error(dx_num, dx)\n",
    "print 'dw 误差: ', rel_error(dw_num, dw)\n",
    "print 'db 误差: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x = np.random.randn(2, 3, 8, 8)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "out, cache = conv_relu_forward(x, w, b, conv_param)\n",
    "dx, dw, db = conv_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_forward(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_forward(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_forward(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "print '测试 conv_relu:'\n",
    "print 'dx 误差: ', rel_error(dx_num, dx)\n",
    "print 'dw 误差: ', rel_error(dw_num, dw)\n",
    "print 'db 误差: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浅层卷积网络\n",
    "\n",
    "打开 `classifiters\\chapter6\\cnn.py` 文件, 实现`ThreeLayerConvNet`类，完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数检验\n",
    "\n",
    "在不添加正则化的情况下，c分类任务，初始时的损失值应该接近于`log(c)`，运行下列代码进行损失值检验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print '初始损失值所对应分类 (无正则化): ', np.exp(loss)\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print '初始损失值 (正则化): ', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度检验\n",
    "\n",
    "运行下列代码进行梯度检验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(\n",
    "        f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s 最大相对误差: %e' % (param_name, rel_error(\n",
    "            param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过拟合小量数据\n",
    "\n",
    "运行下列代码，确保在小数据集上出现明显的过拟合现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "trainer = Trainer(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                updater_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=5)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss',fontsize=18)\n",
    "plt.plot(trainer.loss_history, 'o')\n",
    "plt.xlabel('iteration',fontsize=18)\n",
    "plt.ylabel('loss',fontsize=18)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(left=0.08, right=0.95, wspace=0.25, hspace=0.3)\n",
    "plt.title('train accuracy VS val accuracy',fontsize=18)\n",
    "plt.plot(trainer.train_acc_history, '-o')\n",
    "plt.plot(trainer.val_acc_history, '-*')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch',fontsize=18)\n",
    "plt.ylabel('accuracy',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 空间批量归一化\n",
    "\n",
    "BN算法是一种非常高效的技术，大大加快的网络训练。但BN算法通常使用在全连接网络，因此在卷积网络中会有一点修改，我们将其称为\"spatial batch normalization.\"\n",
    "\n",
    "正常的BN接收(N,D)数据，现在我们需要将BN算法调整为接受(N,C,H,W)数据。\n",
    "由于之前我们已经编写好了BN算法，你只需要将图片数据重塑成(N,D)然后调用我们实现好了的BN算法，之后再将其输出结果重塑回(N,C,H,W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBN前向传播\n",
    "\n",
    "打开 `classifiters\\chapter6\\cnn_layers.py` 文件, 实现`spatial_batchnorm_forward`，完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#训练阶段：SBN前向传播\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "\n",
    "print '使用BN之前:'\n",
    "print '  数据形状: ', x.shape\n",
    "print '  均值: ', x.mean(axis=(0, 2, 3))\n",
    "print '  标准差: ', x.std(axis=(0, 2, 3))\n",
    "\n",
    "\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "bn_param = {'mode': 'train'}\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print '使用BN后:'\n",
    "print '  输出数据形状: ', out.shape\n",
    "print '  均值: ', out.mean(axis=(0, 2, 3))\n",
    "print '  标准差: ', out.std(axis=(0, 2, 3))\n",
    "\n",
    "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print '在BN后使用(gamma, beta)进行缩放:'\n",
    "print '  输出数据形状: ', out.shape\n",
    "print '  均值: ', out.mean(axis=(0, 2, 3))\n",
    "print '  标准差: ', out.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#测试阶段：SBN前向传播\n",
    "N, C, H, W = 10, 4, 11, 12\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(C)\n",
    "beta = np.zeros(C)\n",
    "for t in xrange(50):\n",
    "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "  spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "print '  均值: ', a_norm.mean(axis=(0, 2, 3))\n",
    "print '  标准差: ', a_norm.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBN反向传播\n",
    "打开 `classifiters\\chapter6\\cnn.py` 文件, 实现`spatial_batchnorm_backward`，完成后运行下列代码进行测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "gamma = np.random.randn(C)\n",
    "beta = np.random.randn(C)\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)\n",
    "print 'dx 误差: ', rel_error(dx_num, dx)\n",
    "print 'dgamma 误差: ', rel_error(da_num, dgamma)\n",
    "print 'dbeta 误差: ', rel_error(db_num, dbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练卷积网络\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=200, reg=0.001)\n",
    "\n",
    "trainer = Trainer(model, data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                updater_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=2)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
