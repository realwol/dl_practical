#-*- coding: utf-8 -*-
import numpy as np

def rnn_step_forward(x, prev_h, Wx, Wh, b):
  """
  RNN单步前向传播，使用tanh激活单元
  Inputs:
  - x: 当前时间步数据输入(N, D).
  - prev_h: 前一时间步隐藏层状态 (N, H)
  - Wx: 输入层到隐藏层连接权重(D, H)
  - Wh:隐藏层到隐藏层连接权重(H, H)
  - b: 隐藏层偏置项(H,)

  Returns 元组:
  - next_h: 下一隐藏层状态(N, H)
  - cache: 缓存
  """
  next_h, cache = None, None
  ##############################################################################
  #                 任务：实现RNN单步前向传播                                  #
  #               将输出值储存在next_h中，                                     #
  #         将反向传播时所需的各项缓存存放在cache中                            #
  ##############################################################################

  
  
  
  ##############################################################################
  #                             结束编码                                       #
  ##############################################################################
  return next_h, cache


def rnn_step_backward(dnext_h, cache):
  """
  RNN单步反向传播。
  Inputs:
  - dnext_h: 后一时间片段的梯度。
  - cache: 前向传播时的缓存。
  
  Returns 元组:
  - dx: 数据梯度(N, D)。
  - dprev_h: 前一时间片段梯度(N, H)。
  - dWx: 输入层到隐藏层权重梯度(D,H)。
  - dWh:  隐藏层到隐藏层权重梯度(H, H)。
  - db: 偏置项梯度(H,)。
  """
  dx, dprev_h, dWx, dWh, db = None, None, None, None, None
  ##############################################################################
  #              任务：实现RNN单步反向传播                                     #
  #      提示：tanh(x)梯度:  1 - tanh(x)*tanh(x)                               # 
  ##############################################################################
  

  
  
  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################
  return dx, dprev_h, dWx, dWh, db


def rnn_forward(x, h0, Wx, Wh, b):
  """
  RNN前向传播。
  Inputs:
  - x: 完整的时序数据 (N, T, D)。
  - h0: 隐藏层初始化状态 (N, H)。
  - Wx: 输入层到隐藏层权重 (D, H)。
  - Wh:  隐藏层到隐藏层权重(H, H)。
  - b: 偏置项(H,)。
  
  Returns 元组:
  - h: 所有时间步隐藏层状态(N, T, H)。
  - cache: 反向传播所需的缓存。
  """
  h, cache = None, None
  ##############################################################################
  #                     任务：实现RNN前向传播。                                #
  #        提示： 使用前面实现的rnn_step_forward 函数。                        #
  ##############################################################################

  
  
  
  ##############################################################################
  #                           结束编码                                         #
  ##############################################################################
  return h, cache


def rnn_backward(dh, cache):
  """
  RNN反向传播。
  Inputs:
  - dh: 隐藏层所有时间步梯度(N, T, H)。
  Returns 元组:
  - dx: 输入数据时序梯度(N, T, D)。
  - dh0: 初始隐藏层梯度(N, H)。
  - dWx: 输入层到隐藏层权重梯度(D, H)。
  - dWh: 隐藏层到隐藏层权重梯度(H, H)。
  - db: 偏置项梯度(H,)。
  """
  dx, dh0, dWx, dWh, db = None, None, None, None, None
  ##############################################################################
  #                任务：实现RNN反向传播。                                     #
  #            提示：使用 rnn_step_backward函数。                              #
  ##############################################################################

  
  
  
  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################
  return dx, dh0, dWx, dWh, db


def word_embedding_forward(x, W):
  """
  词嵌入前向传播，将数据矩阵中的N条长度为T的词索引转化为词向量。
  如：W[x[i,j]]表示第i条，第j时间步单词索引所对应的词向量。
  Inputs:
  - x: 整数型数组(N,T),N表示数据条数，T表示单条数据长度，
    数组的每一元素存放着单词索引，取值范围[0,V)。
  - W: 词向量矩阵(V,D)存放各单词对应的向量。
  
  Returns 元组:
  - out:输出词向量(N, T, D)。 
  - cache:反向传播时所需的缓存。
  """
  out, cache = None, None
  ##############################################################################
  #                     任务：实现词嵌入前向传播。                             #
  ##############################################################################

  
  
  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################
  return out, cache


def word_embedding_backward(dout, cache):
  """
  词嵌入反向传播
  
  Inputs:
  - dout: 上层梯度 (N, T, D)
  - cache:前向传播缓存
  
  Returns:
  - dW: 词嵌入矩阵梯度(V, D).
  """
  dW = None
  ##############################################################################
  #                 任务：实现词嵌入反向传播                                   #
  #             提示：你可以使用np.add.at函数                                  #
  #         例如 np.add.at(a,[1,2],1)相当于a[1],a[2]分别加1                    #
  ##############################################################################

  
  
  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################
  return dW


def sigmoid(x):
  """
  数值稳定版本的sigmoid函数。
  """
  pos_mask = (x >= 0)
  neg_mask = (x < 0)
  z = np.zeros_like(x)
  z[pos_mask] = np.exp(-x[pos_mask])
  z[neg_mask] = np.exp(x[neg_mask])
  top = np.ones_like(x)
  top[neg_mask] = z[neg_mask]
  return top / (1 + z)


def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):
  """
  LSTM单步前向传播
  
  Inputs:
  - x: 输入数据 (N, D)
  - prev_h: 前一隐藏层状态 (N, H)
  - prev_c: 前一细胞状态(N, H)
  - Wx: 输入层到隐藏层权重(D, 4H)
  - Wh: 隐藏层到隐藏层权重 (H, 4H)
  - b: 偏置项(4H,)
  
  Returns 元组:
  - next_h:  下一隐藏层状态(N, H)
  - next_c:  下一细胞状态(N, H)
  - cache: 反向传播所需的缓存
  """
  next_h, next_c, cache = None, None, None
  #############################################################################
  #              任务：实现LSTM单步前向传播。                                 #
  #         提示：稳定版本的sigmoid函数已经帮你实现，直接调用即可。           #
  #               tanh函数使用np.tanh。                                       #
  #############################################################################

  
  
  
  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################
  
  return next_h, next_c, cache


def lstm_step_backward(dnext_h, dnext_c, cache):
  """
   LSTM单步反向传播
  
  Inputs:
  - dnext_h: 下一隐藏层梯度 (N, H)
  - dnext_c: 下一细胞梯度 (N, H)
  - cache: 前向传播缓存
  
  Returns 元组:
  - dx: 输入数据梯度 (N, D)
  - dprev_h: 前一隐藏层梯度 (N, H)
  - dprev_c: 前一细胞梯度(N, H)
  - dWx: 输入层到隐藏层梯度(D, 4H)
  - dWh:  隐藏层到隐藏层梯度(H, 4H)
  - db:  偏置梯度(4H,)
  """
  dx, dprev_h, dc, dWx, dWh, db = None, None, None, None, None, None
  #############################################################################
  #                           任务：实现LSTM单步反向传播                      #
  #          提示：sigmoid(x)函数梯度：sigmoid(x)*(1-sigmoid(x))              #
  #                tanh(x)函数梯度：   1-tanh(x)*tanh(x)                      #
  #############################################################################

  
  
  
  
  
  
  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################

  return dx, dprev_h, dprev_c, dWx, dWh, db


def lstm_forward(x, h0, Wx, Wh, b):
  """
  LSTM前向传播
  Inputs:
  - x: 输入数据 (N, T, D)
  - h0:初始化隐藏层状态(N, H)
  - Wx: 输入层到隐藏层权重 (D, 4H)
  - Wh: 隐藏层到隐藏层权重(H, 4H)
  - b: 偏置项(4H,)
  
  Returns 元组:
  - h: 隐藏层所有状态 (N, T, H)
  - cache: 用于反向传播的缓存
  """
  h, cache = None, None
  #############################################################################
  #                    任务： 实现完整的LSTM前向传播                          #
  #############################################################################

  
  
  

  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################

  return h, cache


def lstm_backward(dh, cache):
  """
  LSTM反向传播
  Inputs:
  - dh: 各隐藏层梯度(N, T, H)
  - cache: V前向传播缓存
  
  Returns 元组:
  - dx: 输入数据梯度 (N, T, D)
  - dh0:初始隐藏层梯度(N, H)
  - dWx: 输入层到隐藏层权重梯度 (D, 4H)
  - dWh: 隐藏层到隐藏层权重梯度 (H, 4H)
  - db: 偏置项梯度 (4H,)
  """
  dx, dh0, dWx, dWh, db = None, None, None, None, None
  #############################################################################
  #                 任务：实现完整的LSTM反向传播                              #
  #############################################################################

  
  
  
  
  ##############################################################################
  #                               结束编码                                     #
  ##############################################################################
  
  return dx, dh0, dWx, dWh, db


def temporal_affine_forward(x, w, b):
  """
  时序隐藏层仿射传播：将隐藏层时序数据(N,T,D)重塑为(N*T,D)，
  完成前向传播后，再重塑回原型输出。

  Inputs:
  - x: 时序数据(N, T, D)。
  - w: 权重(D, M)。
  - b: 偏置(M,)。
  
  Returns 元组:
  - out: 输出(N, T, M)。
  - cache: 反向传播缓存。
  """
  N, T, D = x.shape
  M = b.shape[0]
  out = x.reshape(N * T, D).dot(w).reshape(N, T, M) + b
  cache = x, w, b, out
  return out, cache


def temporal_affine_backward(dout, cache):
  """
  时序隐藏层仿射反向传播。

  Input:
  - dout:上层梯度 (N, T, M)。
  - cache: 前向传播缓存。

  Returns 元组:
  - dx: 输入梯度(N, T, D)。
  - dw: 权重梯度 (D, M)。
  - db: 偏置项梯度 (M,)。
  """
  x, w, b, out = cache
  N, T, D = x.shape
  M = b.shape[0]

  dx = dout.reshape(N * T, M).dot(w.T).reshape(N, T, D)
  dw = dout.reshape(N * T, M).T.dot(x.reshape(N * T, D)).T
  db = dout.sum(axis=(0, 1))

  return dx, dw, db


def temporal_softmax_loss(x, y, mask, verbose=False):
  """
  时序版本的Softmax损失和原版本类似，只需将数据(N, T, V)重塑为(N*T,V)即可。
  需要注意的是，对于NULL标记不计入损失值，因此，你需要加入掩码进行过滤。
  Inputs:
  - x: 输入数据得分(N, T, V)。
  - y: 目标索引(N, T)，其中0<= y[i, t] < V。
  - mask: 过滤NULL标记的掩码。
  Returns 元组:
  - loss: 损失值。
  - dx: x梯度。
  """

  N, T, V = x.shape
  
  x_flat = x.reshape(N * T, V)
  y_flat = y.reshape(N * T)
  mask_flat = mask.reshape(N * T)
  
  probs = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))
  probs /= np.sum(probs, axis=1, keepdims=True)
  loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N
  dx_flat = probs.copy()
  dx_flat[np.arange(N * T), y_flat] -= 1
  dx_flat /= N
  dx_flat *= mask_flat[:, None]
  
  if verbose: print 'dx_flat: ', dx_flat.shape
  
  dx = dx_flat.reshape(N, T, V)
  
  return loss, dx

